{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = str(os.getcwd()) + \"\\\\data\"\n",
    "\n",
    "input_filepaths = [\"\\\\pre_depression.csv\", \"\\\\post_depression.csv\", \"\\\\post_finance.csv\", \"\\\\post_fitness.csv\", \"\\\\post_jokes.csv\"]\n",
    "output_filepaths = [\"\\\\preproc_predepression.csv\", \"\\\\preproc_postdepression.csv\", \"\\\\preproc_postfinance.csv\", \"\\\\preproc_postfitness.csv\", \"\\\\preproc_postjokes.csv\"]\n",
    "train_fp = \"\\\\train.csv\"\n",
    "validation_fp = \"\\\\validation.csv\"\n",
    "test_fp = \"\\\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    try:\n",
    "        # Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Please provide a valid file path.\")\n",
    "        return None\n",
    "\n",
    "def save_data(df, output_file):\n",
    "    try:\n",
    "        # Save the processed data to a new CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed data saved to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving the processed data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract_doc(doc):\n",
    "    if doc == \"nan\" or doc == \"\":\n",
    "        return \"\"\n",
    "    clean_words = []\n",
    "\n",
    "    for word in doc.split():\n",
    "        #Remove special characters\n",
    "        word_wo_sp = re.sub(\"[^a-zA-Z' ]\", \"\", word)\n",
    "        \n",
    "        # using contractions.fix to expand the shortened words\n",
    "        phrases = contractions.fix(word_wo_sp)\n",
    "\n",
    "        for word in phrases.split():\n",
    "            clean_words.append(word)\n",
    "\n",
    "    #Filter out empty strings after regex replacement\n",
    "    clean_words = filter(None, clean_words)\n",
    "    \n",
    "    #Concatenate back into string\n",
    "    clean_string = ' '.join(char for char in clean_words)\n",
    "    return clean_string\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    if doc == \"nan\" or doc == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    word_tokens = word_tokenize(doc)\n",
    "    #Try changing stopword corpus to include negation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = ' '.join(char for char in filtered_sentence)\n",
    "    if filtered_sentence == '':\n",
    "        return \"\"\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: \\pre_depression.csv\n",
      "Done processing file: \\pre_depression.csv\n",
      "Processing file: \\post_depression.csv\n",
      "Done processing file: \\post_depression.csv\n",
      "Processing file: \\post_finance.csv\n",
      "Done processing file: \\post_finance.csv\n",
      "Processing file: \\post_fitness.csv\n",
      "Done processing file: \\post_fitness.csv\n",
      "Processing file: \\post_jokes.csv\n",
      "Done processing file: \\post_jokes.csv\n"
     ]
    }
   ],
   "source": [
    "overall_df = pd.DataFrame()\n",
    "training_split = 0.7\n",
    "validation_split = 0.15\n",
    "testing_split = 0.15\n",
    "\n",
    "for i in range(len(input_filepaths)):\n",
    "    print(\"Processing file: \" + input_filepaths[i])\n",
    "    input_fp = main_filepath + input_filepaths[i]\n",
    "    output_fp = main_filepath + output_filepaths[i]\n",
    "    processed_df = pd.DataFrame()\n",
    "    df = load_data(input_fp)\n",
    "\n",
    "    processed_df['subreddit'] = df['subreddit']\n",
    "    processed_df['post'] = df['post'].apply(lambda x: remove_stopwords(contract_doc(x)))\n",
    "    processed_df['label'] = df['subreddit'].apply(lambda x: 1 if x == 'depression' else 0)\n",
    "    \n",
    "    # Save the processed DataFrame to a new CSV file\n",
    "    processed_df.to_csv(output_fp, index=False)   \n",
    "    overall_df = pd.concat([overall_df, processed_df], ignore_index=True)\n",
    "    print(\"Done processing file: \" + input_filepaths[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "train_df, test_df = train_test_split(overall_df, test_size=1-training_split, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=validation_split/(1.-testing_split), random_state=42)\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "train_df.to_csv(main_filepath + train_fp, index=False) \n",
    "val_df.to_csv(main_filepath + validation_fp, index=False) \n",
    "test_df.to_csv(main_filepath + test_fp, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: TfIdf vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = str(os.getcwd()) + \"\\\\data\"\n",
    "\n",
    "#Tfidf based on entire training sets\n",
    "train_input = \"\\\\train.csv\"\n",
    "validation_input = \"\\\\validation.csv\"\n",
    "test_input = \"\\\\test.csv\"\n",
    "\n",
    "train_count = \"\\\\train_count.csv\"\n",
    "validation_count = \"\\\\validation_count.csv\"\n",
    "test_count = \"\\\\test_count.csv\"\n",
    "\n",
    "train_tfidf = \"\\\\train_tfidf.csv\"\n",
    "validation_tfidf = \"\\\\validation_tfidf.csv\"\n",
    "test_tfidf = \"\\\\test_tfidf.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    try:\n",
    "        # Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Please provide a valid file path.\")\n",
    "        return None\n",
    "\n",
    "def save_data(df, output_file):\n",
    "    try:\n",
    "        # Save the processed data to a new CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed data saved to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving the processed data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering Statistical Information: \\train.csv\n",
      "Done gathering Statistical Information: \\train.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Gathering Statistical Information: \" + train_input)\n",
    "train_input_fp = main_filepath + train_input\n",
    "train_output_count = main_filepath + train_count\n",
    "train_output_tfidf = main_filepath + train_tfidf\n",
    "\n",
    "train_df = load_data(train_input_fp)\n",
    "    \n",
    "# Create the TfidfVectorizer\n",
    "train_count_vectorizer = CountVectorizer(analyzer= 'word', stop_words='english', max_features=2048)\n",
    "train_tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english', max_features=2048)\n",
    "\n",
    "# Fit and transform the documents\n",
    "train_count_wm = train_count_vectorizer.fit_transform(train_df['post'].values.astype(str))\n",
    "train_tfidf_wm = train_tfidf_vectorizer.fit_transform(train_df['post'].values.astype(str))\n",
    "\n",
    "# Get feature names (words) and IDF values\n",
    "train_count_tokens = train_count_vectorizer.get_feature_names_out()\n",
    "train_tfidf_tokens = train_tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "traindf_countvect = pd.DataFrame(data = train_count_wm.toarray(), columns = train_count_tokens)\n",
    "traindf_countvect = pd.concat([train_df, traindf_countvect], axis=1)\n",
    "\n",
    "traindf_tfidfvect = pd.DataFrame(data = train_tfidf_wm.toarray(), columns = train_tfidf_tokens)\n",
    "traindf_tfidfvect = pd.concat([train_df, traindf_tfidfvect], axis=1)\n",
    "\n",
    "traindf_countvect.to_csv(train_output_count, index=False)\n",
    "traindf_tfidfvect.to_csv(train_output_tfidf, index=False)\n",
    "\n",
    "print(\"Done gathering Statistical Information: \" + train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering Statistical Information: \\test.csv\n",
      "Done gathering Statistical Information: \\test.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Gathering Statistical Information: \" + test_input)\n",
    "test_input_fp = main_filepath + test_input\n",
    "test_output_count = main_filepath + test_count\n",
    "test_output_tfidf = main_filepath + test_tfidf\n",
    "\n",
    "test_df = load_data(test_input_fp)\n",
    "    \n",
    "# Create the TfidfVectorizer\n",
    "test_count_vectorizer = CountVectorizer(analyzer= 'word', stop_words='english', max_features=2048)\n",
    "test_tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english', max_features=2048)\n",
    "\n",
    "# Fit and transform the documents\n",
    "test_count_wm = test_count_vectorizer.fit_transform(test_df['post'].values.astype(str))\n",
    "test_tfidf_wm = test_tfidf_vectorizer.fit_transform(test_df['post'].values.astype(str))\n",
    "\n",
    "# Get feature names (words) and IDF values\n",
    "test_count_tokens = test_count_vectorizer.get_feature_names_out()\n",
    "test_tfidf_tokens = test_tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "testdf_countvect = pd.DataFrame(data = test_count_wm.toarray(), columns = test_count_tokens)\n",
    "testdf_countvect = pd.concat([test_df, testdf_countvect], axis=1)\n",
    "\n",
    "testdf_tfidfvect = pd.DataFrame(data = test_tfidf_wm.toarray(), columns = test_tfidf_tokens)\n",
    "testdf_tfidfvect = pd.concat([test_df, testdf_tfidfvect], axis=1)\n",
    "\n",
    "testdf_countvect.to_csv(test_output_count, index=False)\n",
    "testdf_tfidfvect.to_csv(test_output_tfidf, index=False)\n",
    "\n",
    "print(\"Done gathering Statistical Information: \" + test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Embedding and Dot Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = str(os.getcwd()) + \"\\\\data\"\n",
    "\n",
    "#Tfidf based on entire training sets\n",
    "train_input = \"\\\\train_tfidf.csv\"\n",
    "validation_input = \"\\\\validation_tfidf.csv\"\n",
    "test_input = \"\\\\test_tfidf.csv\"\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model_path = 'C:\\\\Users\\\\benny\\\\Desktop\\\\Y4S1\\\\Natural_Language_Processing\\\\project\\\\pretrained_word2vec\\\\en_wiki_word2vec_300\\\\en_wiki_word2vec_300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_m = KeyedVectors.load_word2vec_format(model_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector for a specific word\n",
    "def get_embedding(word, word2vec_model):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return np.zeros((300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec loaded. Generating word embedding: \\train_tfidf.csv\n",
      "(2048, 300)\n",
      "Training embedding generated: \\train_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2vec loaded. Generating word embedding: \" + train_input)\n",
    "train_input_fp = main_filepath + train_input\n",
    "train_df = pd.read_csv(train_input_fp)\n",
    "\n",
    "train_embed_vect = []\n",
    "train_col_headers = train_df.columns.tolist()\n",
    "train_col_headers = train_col_headers[3:]\n",
    "\n",
    "for header in train_col_headers:\n",
    "    header_embedding = get_embedding(header, w2v_m)\n",
    "    flat_embed = header_embedding.flatten()\n",
    "    train_embed_vect.append(flat_embed)\n",
    "print(np.shape(train_embed_vect))\n",
    "print(\"Training embedding generated: \" + train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76921, 2048)\n",
      "(76921, 300)\n",
      "Training feature extracted: \\train_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "# Getting the tfidf matrix\n",
    "train_tfidf_mat = train_df.iloc[:, 3:]\n",
    "print(np.shape(train_tfidf_mat))\n",
    "\n",
    "# Features (document) are stacked row by row in the matrix\n",
    "train_vect_rep = np.matmul(train_tfidf_mat, train_embed_vect)\n",
    "print(np.shape(train_vect_rep))\n",
    "\n",
    "train_label = train_df['label'].values.flatten()\n",
    "print(\"Training feature extracted: \" + train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding: \\test_tfidf.csv\n",
      "(2048, 300)\n",
      "Test embedding generated: \\test_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating word embedding: \" + test_input)\n",
    "test_input_fp = main_filepath + test_input\n",
    "test_df = pd.read_csv(test_input_fp)\n",
    "\n",
    "test_embed_vect = []\n",
    "test_col_headers = test_df.columns.tolist()\n",
    "test_col_headers = test_col_headers[3:]\n",
    "\n",
    "for header in test_col_headers:\n",
    "    header_embedding = get_embedding(header, w2v_m)\n",
    "    flat_embed = header_embedding.flatten()\n",
    "    test_embed_vect.append(flat_embed)\n",
    "print(np.shape(test_embed_vect))\n",
    "print(\"Test embedding generated: \" + test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40031, 2048)\n",
      "(40031, 300)\n",
      "Testing feature extracted: \\train_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "# Getting the tfidf matrix\n",
    "test_tfidf_mat = test_df.iloc[:, 3:]\n",
    "print(np.shape(test_tfidf_mat))\n",
    "\n",
    "# Features (document) are stacked row by row in the matrix\n",
    "test_vect_rep = np.matmul(test_tfidf_mat, test_embed_vect)\n",
    "print(np.shape(test_vect_rep))\n",
    "\n",
    "test_label = test_df['label'].values.flatten()\n",
    "print(\"Testing feature extracted: \" + test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting data into SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = str(os.getcwd()) + \"\\\\data\"\n",
    "\n",
    "test_input = \"\\\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fp = main_filepath + test_input\n",
    "test_df = pd.read_csv(test_input_fp)\n",
    "\n",
    "classifier = svm.SVC(kernel = 'linear')\n",
    "classifier.fit(train_vect_rep, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = classifier.predict(test_vect_rep)\n",
    "test_label = test_df['label'].values.flatten()\n",
    "test_df['predict'] = label_pred\n",
    "test_df.to_csv(main_filepath + \"\\\\final_linear_svm.csv\", index=False)\n",
    "\n",
    "cm = confusion_matrix(test_label, label_pred)\n",
    "print(cm)\n",
    "accuracy_score(test_label, label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fp = main_filepath + test_input\n",
    "test_df = pd.read_csv(test_input_fp)\n",
    "\n",
    "classifier2 = svm.SVC(kernel = 'rbf', gamma=0.1)\n",
    "classifier2.fit(train_vect_rep, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = classifier2.predict(test_vect_rep)\n",
    "test_label = test_df['label'].values.flatten()\n",
    "test_df['predict'] = label_pred\n",
    "test_df.to_csv(main_filepath + \"\\\\final_rbf_svm.csv\", index=False)\n",
    "\n",
    "cm = confusion_matrix(test_label, label_pred)\n",
    "print(cm)\n",
    "accuracy_score(test_label, label_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
