{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = str(os.getcwd()) + \"\\\\data\"\n",
    "\n",
    "input_filepaths = [\"\\\\pre_depression.csv\", \"\\\\post_depression.csv\", \"\\\\post_finance.csv\", \"\\\\post_fitness.csv\", \"\\\\post_jokes.csv\"]\n",
    "output_filepaths = [\"\\\\preproc_predepression.csv\", \"\\\\preproc_postdepression.csv\", \"\\\\preproc_postfinance.csv\", \"\\\\preproc_postfitness.csv\", \"\\\\preproc_postjokes.csv\"]\n",
    "train_fp = \"\\\\train.csv\"\n",
    "validation_fp = \"\\\\validation.csv\"\n",
    "test_fp = \"\\\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    try:\n",
    "        # Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Please provide a valid file path.\")\n",
    "        return None\n",
    "\n",
    "def save_data(df, output_file):\n",
    "    try:\n",
    "        # Save the processed data to a new CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed data saved to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving the processed data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract_doc(doc):\n",
    "    if doc == \"nan\" or doc == \"\":\n",
    "        return \"\"\n",
    "    clean_words = []\n",
    "\n",
    "    for word in doc.split():\n",
    "        #Remove special characters\n",
    "        word_wo_sp = re.sub(\"[^a-zA-Z' ]\", \"\", word)\n",
    "        \n",
    "        # using contractions.fix to expand the shortened words\n",
    "        phrases = contractions.fix(word_wo_sp)\n",
    "\n",
    "        for word in phrases.split():\n",
    "            clean_words.append(word)\n",
    "\n",
    "    #Filter out empty strings after regex replacement\n",
    "    clean_words = filter(None, clean_words)\n",
    "    \n",
    "    #Concatenate back into string\n",
    "    clean_string = ' '.join(char for char in clean_words)\n",
    "    return clean_string\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    if doc == \"nan\" or doc == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    word_tokens = word_tokenize(doc)\n",
    "    #Try changing stopword corpus to include negation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = ' '.join(char for char in filtered_sentence)\n",
    "    if filtered_sentence == '':\n",
    "        return \"\"\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: \\pre_depression.csv\n",
      "Done processing file: \\pre_depression.csv\n",
      "Processing file: \\post_depression.csv\n",
      "Done processing file: \\post_depression.csv\n",
      "Processing file: \\post_finance.csv\n",
      "Done processing file: \\post_finance.csv\n",
      "Processing file: \\post_fitness.csv\n",
      "Done processing file: \\post_fitness.csv\n",
      "Processing file: \\post_jokes.csv\n",
      "Done processing file: \\post_jokes.csv\n"
     ]
    }
   ],
   "source": [
    "overall_df = pd.DataFrame()\n",
    "training_split = 0.7\n",
    "validation_split = 0.15\n",
    "testing_split = 0.15\n",
    "\n",
    "for i in range(len(input_filepaths)):\n",
    "    print(\"Processing file: \" + input_filepaths[i])\n",
    "    input_fp = main_filepath + input_filepaths[i]\n",
    "    output_fp = main_filepath + output_filepaths[i]\n",
    "    processed_df = pd.DataFrame()\n",
    "    df = load_data(input_fp)\n",
    "\n",
    "    processed_df['subreddit'] = df['subreddit']\n",
    "    processed_df['post'] = df['post'].apply(lambda x: remove_stopwords(contract_doc(x)))\n",
    "    processed_df['label'] = df['subreddit'].apply(lambda x: 1 if x == 'depression' else 0)\n",
    "    \n",
    "    # Save the processed DataFrame to a new CSV file\n",
    "    processed_df.to_csv(output_fp, index=False)   \n",
    "    overall_df = pd.concat([overall_df, processed_df], ignore_index=True)\n",
    "    print(\"Done processing file: \" + input_filepaths[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "train_df, test_df = train_test_split(overall_df, test_size=1-training_split, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=validation_split/(1.-testing_split), random_state=42)\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "train_df.to_csv(main_filepath + train_fp, index=False) \n",
    "val_df.to_csv(main_filepath + validation_fp, index=False) \n",
    "test_df.to_csv(main_filepath + test_fp, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: TfIdf vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = str(os.getcwd()) + \"\\\\data\"\n",
    "\n",
    "#Tfidf based on entire training sets\n",
    "train_input = \"\\\\train.csv\"\n",
    "validation_input = \"\\\\validation.csv\"\n",
    "test_input = \"\\\\test.csv\"\n",
    "\n",
    "train_count = \"\\\\train_count.csv\"\n",
    "validation_count = \"\\\\validation_count.csv\"\n",
    "test_count = \"\\\\test_count.csv\"\n",
    "\n",
    "train_tfidf = \"\\\\train_tfidf.csv\"\n",
    "validation_tfidf = \"\\\\validation_tfidf.csv\"\n",
    "test_tfidf = \"\\\\test_tfidf.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    try:\n",
    "        # Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Please provide a valid file path.\")\n",
    "        return None\n",
    "\n",
    "def save_data(df, output_file):\n",
    "    try:\n",
    "        # Save the processed data to a new CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed data saved to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving the processed data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering Statistical Information: \\train.csv\n",
      "Done gathering Statistical Information: \\train.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Gathering Statistical Information: \" + train_input)\n",
    "train_input_fp = main_filepath + train_input\n",
    "train_output_count = main_filepath + train_count\n",
    "train_output_tfidf = main_filepath + train_tfidf\n",
    "\n",
    "train_df = load_data(train_input_fp)\n",
    "    \n",
    "# Create the TfidfVectorizer\n",
    "train_count_vectorizer = CountVectorizer(analyzer= 'word', stop_words='english', max_features=2048)\n",
    "train_tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english', max_features=2048)\n",
    "\n",
    "# Fit and transform the documents\n",
    "train_count_wm = train_count_vectorizer.fit_transform(train_df['post'].values.astype(str))\n",
    "train_tfidf_wm = train_tfidf_vectorizer.fit_transform(train_df['post'].values.astype(str))\n",
    "\n",
    "# Get feature names (words) and IDF values\n",
    "train_count_tokens = train_count_vectorizer.get_feature_names_out()\n",
    "train_tfidf_tokens = train_tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "traindf_countvect = pd.DataFrame(data = train_count_wm.toarray(), columns = train_count_tokens)\n",
    "traindf_countvect = pd.concat([train_df, traindf_countvect], axis=1)\n",
    "\n",
    "traindf_tfidfvect = pd.DataFrame(data = train_tfidf_wm.toarray(), columns = train_tfidf_tokens)\n",
    "traindf_tfidfvect = pd.concat([train_df, traindf_tfidfvect], axis=1)\n",
    "\n",
    "traindf_countvect.to_csv(train_output_count, index=False)\n",
    "traindf_tfidfvect.to_csv(train_output_tfidf, index=False)\n",
    "\n",
    "print(\"Done gathering Statistical Information: \" + train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering Statistical Information: \\test.csv\n",
      "Done gathering Statistical Information: \\test.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Gathering Statistical Information: \" + test_input)\n",
    "test_input_fp = main_filepath + test_input\n",
    "test_output_count = main_filepath + test_count\n",
    "test_output_tfidf = main_filepath + test_tfidf\n",
    "\n",
    "test_df = load_data(test_input_fp)\n",
    "    \n",
    "# Create the TfidfVectorizer\n",
    "test_count_vectorizer = CountVectorizer(analyzer= 'word', stop_words='english', max_features=2048)\n",
    "test_tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english', max_features=2048)\n",
    "\n",
    "# Fit and transform the documents\n",
    "test_count_wm = test_count_vectorizer.fit_transform(test_df['post'].values.astype(str))\n",
    "test_tfidf_wm = test_tfidf_vectorizer.fit_transform(test_df['post'].values.astype(str))\n",
    "\n",
    "# Get feature names (words) and IDF values\n",
    "test_count_tokens = test_count_vectorizer.get_feature_names_out()\n",
    "test_tfidf_tokens = test_tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "testdf_countvect = pd.DataFrame(data = test_count_wm.toarray(), columns = test_count_tokens)\n",
    "testdf_countvect = pd.concat([test_df, testdf_countvect], axis=1)\n",
    "\n",
    "testdf_tfidfvect = pd.DataFrame(data = test_tfidf_wm.toarray(), columns = test_tfidf_tokens)\n",
    "testdf_tfidfvect = pd.concat([test_df, testdf_tfidfvect], axis=1)\n",
    "\n",
    "testdf_countvect.to_csv(test_output_count, index=False)\n",
    "testdf_tfidfvect.to_csv(test_output_tfidf, index=False)\n",
    "\n",
    "print(\"Done gathering Statistical Information: \" + test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Embedding and Dot Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = str(os.getcwd()) + \"\\\\data\"\n",
    "\n",
    "#Tfidf based on entire training sets\n",
    "train_input = \"\\\\train_tfidf.csv\"\n",
    "validation_input = \"\\\\validation_tfidf.csv\"\n",
    "test_input = \"\\\\test_tfidf.csv\"\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model_path = 'C:\\\\Users\\\\benny\\\\Desktop\\\\Y4S1\\\\Natural_Language_Processing\\\\project\\\\pretrained_word2vec\\\\en_wiki_word2vec_300\\\\en_wiki_word2vec_300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_m = KeyedVectors.load_word2vec_format(model_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector for a specific word\n",
    "def get_embedding(word, word2vec_model):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return np.zeros((300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec loaded. Generating word embedding: \\train_tfidf.csv\n",
      "(2048, 300)\n",
      "Training embedding generated: \\train_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2vec loaded. Generating word embedding: \" + train_input)\n",
    "train_input_fp = main_filepath + train_input\n",
    "train_df = pd.read_csv(train_input_fp)\n",
    "\n",
    "train_embed_vect = []\n",
    "train_col_headers = train_df.columns.tolist()\n",
    "train_col_headers = train_col_headers[3:]\n",
    "\n",
    "for header in train_col_headers:\n",
    "    header_embedding = get_embedding(header, w2v_m)\n",
    "    flat_embed = header_embedding.flatten()\n",
    "    train_embed_vect.append(flat_embed)\n",
    "print(np.shape(train_embed_vect))\n",
    "print(\"Training embedding generated: \" + train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76921, 2048)\n",
      "(76921, 300)\n",
      "Training feature extracted: \\train_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "# Getting the tfidf matrix\n",
    "train_tfidf_mat = train_df.iloc[:, 3:]\n",
    "print(np.shape(train_tfidf_mat))\n",
    "\n",
    "# Features (document) are stacked row by row in the matrix\n",
    "train_vect_rep = np.matmul(train_tfidf_mat, train_embed_vect)\n",
    "print(np.shape(train_vect_rep))\n",
    "\n",
    "train_label = train_df['label'].values.flatten()\n",
    "print(\"Training feature extracted: \" + train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding: \\test_tfidf.csv\n",
      "(2048, 300)\n",
      "Test embedding generated: \\test_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating word embedding: \" + test_input)\n",
    "test_input_fp = main_filepath + test_input\n",
    "test_df = pd.read_csv(test_input_fp)\n",
    "\n",
    "test_embed_vect = []\n",
    "test_col_headers = test_df.columns.tolist()\n",
    "test_col_headers = test_col_headers[3:]\n",
    "\n",
    "for header in test_col_headers:\n",
    "    header_embedding = get_embedding(header, w2v_m)\n",
    "    flat_embed = header_embedding.flatten()\n",
    "    test_embed_vect.append(flat_embed)\n",
    "print(np.shape(test_embed_vect))\n",
    "print(\"Test embedding generated: \" + test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40031, 2048)\n",
      "(40031, 300)\n",
      "Testing feature extracted: \\test_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "# Getting the tfidf matrix\n",
    "test_tfidf_mat = test_df.iloc[:, 3:]\n",
    "print(np.shape(test_tfidf_mat))\n",
    "\n",
    "# Features (document) are stacked row by row in the matrix\n",
    "test_vect_rep = np.matmul(test_tfidf_mat, test_embed_vect)\n",
    "print(np.shape(test_vect_rep))\n",
    "\n",
    "test_label = test_df['label'].values.flatten()\n",
    "print(\"Testing feature extracted: \" + test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting data into SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = str(os.getcwd()) + \"\\\\data\"\n",
    "\n",
    "test_input = \"\\\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_fp = main_filepath + test_input\n",
    "test_df = pd.read_csv(test_input_fp)\n",
    "\n",
    "classifier = svm.SVC(kernel = 'linear')\n",
    "classifier.fit(train_vect_rep, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21441   775]\n",
      " [ 1051 16764]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9543853513527016"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pred = classifier.predict(test_vect_rep)\n",
    "test_label = test_df['label'].values.flatten()\n",
    "test_df['predict'] = label_pred\n",
    "test_df.to_csv(main_filepath + \"\\\\final_linear_svm.csv\", index=False)\n",
    "\n",
    "cm = confusion_matrix(test_label, label_pred)\n",
    "print(cm)\n",
    "accuracy_score(test_label, label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(gamma=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(gamma=0.1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(gamma=0.1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_fp = main_filepath + test_input\n",
    "test_df = pd.read_csv(test_input_fp)\n",
    "\n",
    "classifier2 = svm.SVC(kernel = 'rbf', gamma=0.1)\n",
    "classifier2.fit(train_vect_rep, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21687   529]\n",
      " [  767 17048]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96762509055482"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pred = classifier2.predict(test_vect_rep)\n",
    "test_label = test_df['label'].values.flatten()\n",
    "test_df['predict'] = label_pred\n",
    "test_df.to_csv(main_filepath + \"\\\\final_rbf_svm.csv\", index=False)\n",
    "\n",
    "cm2 = confusion_matrix(test_label, label_pred)\n",
    "print(cm2)\n",
    "accuracy_score(test_label, label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20484  1732]\n",
      " [ 4003 13812]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8567360295770777"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(train_vect_rep, train_label)\n",
    "nb_pred = nb_classifier.predict(test_vect_rep)\n",
    "\n",
    "cm3 = confusion_matrix(test_label, nb_pred)\n",
    "print(cm3)\n",
    "accuracy_score(test_label, nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.95\n",
      "Precision: 0.96\n",
      "Recall: 0.94\n",
      "F1 Score: 0.95\n",
      "[[20484  1732]\n",
      " [ 4003 13812]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8567360295770777"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_model = LogisticRegression(max_iter=10000)\n",
    "logreg_model.fit(train_vect_rep, train_label)\n",
    "logreg_pred = logreg_model.predict(test_vect_rep)\n",
    "\n",
    "accuracy = metrics.accuracy_score(test_label, logreg_pred)\n",
    "precision = metrics.precision_score(test_label, logreg_pred)\n",
    "recall = metrics.recall_score(test_label, logreg_pred)\n",
    "f1_score = metrics.f1_score(test_label, logreg_pred)\n",
    "cm4 = confusion_matrix(test_label, logreg_pred)\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Precision:\", round(precision,2))\n",
    "print(\"Recall:\", round(recall,2))\n",
    "print(\"F1 Score:\", round(f1_score,2))\n",
    "print(cm4)\n",
    "accuracy_score(test_label, logreg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\benny\\Desktop\\Y4S1\\Natural_Language_Processing\\project\\preprocess.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benny/Desktop/Y4S1/Natural_Language_Processing/project/preprocess.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mn_nb_classifier \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/benny/Desktop/Y4S1/Natural_Language_Processing/project/preprocess.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m mn_nb_classifier\u001b[39m.\u001b[39;49mfit(train_vect_rep, train_label)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benny/Desktop/Y4S1/Natural_Language_Processing/project/preprocess.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mn_nb_pred \u001b[39m=\u001b[39m mn_nb_classifier\u001b[39m.\u001b[39mpredict(test_vect_rep)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benny/Desktop/Y4S1/Natural_Language_Processing/project/preprocess.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cm4 \u001b[39m=\u001b[39m confusion_matrix(test_label, mn_nb_pred)\n",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:772\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    770\u001b[0m n_classes \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    771\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 772\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count(X, Y)\n\u001b[0;32m    773\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_alpha()\n\u001b[0;32m    774\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:894\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_count\u001b[39m(\u001b[39mself\u001b[39m, X, Y):\n\u001b[0;32m    893\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 894\u001b[0m     check_non_negative(X, \u001b[39m\"\u001b[39;49m\u001b[39mMultinomialNB (input X)\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    895\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_count_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m safe_sparse_dot(Y\u001b[39m.\u001b[39mT, X)\n\u001b[0;32m    896\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_count_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1489\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1486\u001b[0m     X_min \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mmin(X)\n\u001b[0;32m   1488\u001b[0m \u001b[39mif\u001b[39;00m X_min \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1489\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNegative values in data passed to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "#has errors\n",
    "mn_nb_classifier = MultinomialNB()\n",
    "mn_nb_classifier.fit(train_vect_rep, train_label)\n",
    "mn_nb_pred = mn_nb_classifier.predict(test_vect_rep)\n",
    "\n",
    "cm4 = confusion_matrix(test_label, mn_nb_pred)\n",
    "print(cm4)\n",
    "accuracy_score(test_label, mn_nb_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
